{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 397, in _error_catcher\n",
      "    yield\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 479, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"d:\\python\\python36\\lib\\http\\client.py\", line 449, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"d:\\python\\python36\\lib\\http\\client.py\", line 493, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"d:\\python\\python36\\lib\\socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"d:\\python\\python36\\lib\\ssl.py\", line 1009, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"d:\\python\\python36\\lib\\ssl.py\", line 871, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"d:\\python\\python36\\lib\\ssl.py\", line 631, in read\n",
      "    v = self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 345, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 196, in resolve\n",
      "    self._resolve_one(requirement_set, req)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 359, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 307, in _get_abstract_dist_for\n",
      "    self.require_hashes\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 199, in prepare_linked_requirement\n",
      "    progress_bar=self.progress_bar\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 1050, in unpack_url\n",
      "    progress_bar=progress_bar\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 910, in unpack_http_url\n",
      "    progress_bar)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 1138, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 847, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\utils\\hashes.py\", line 75, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 815, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_internal\\download.py\", line 804, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 531, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 496, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"d:\\python\\python36\\lib\\contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"d:\\python\\python36\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 402, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-nightly-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Load dataset.\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tfbt/titanic_train.csv')\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tfbt/titanic_eval.csv')\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预览数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预览统计大纲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对单独特征进行分析预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.age.hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.sex.value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dftrain['class']\n",
    "  .value_counts()\n",
    "  .plot(kind='barh'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dftrain['embark_town']\n",
    "  .value_counts()\n",
    "  .plot(kind='barh'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (pd.concat([dftrain, y_train], axis=1)\\\n",
    "  .groupby('sex')\n",
    "  .survived\n",
    "  .mean()\n",
    "  .plot(kind='barh'))\n",
    "ax.set_xlabel('% survive');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = tf.feature_column\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', \n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "  \n",
    "def one_hot_cat_column(feature_name, vocab):\n",
    "  return fc.indicator_column(\n",
    "      fc.categorical_column_with_vocabulary_list(feature_name,\n",
    "                                                 vocab))\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "  # Need to one-hot encode categorical features.\n",
    "  vocabulary = dftrain[feature_name].unique()\n",
    "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "  \n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(fc.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "独热码的转换效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dftrain.head(1)\n",
    "class_fc = one_hot_cat_column('class', ('First', 'Second', 'Third'))\n",
    "print('Feature value: \"{}\"'.format(example['class'].iloc[0]))\n",
    "print('One-hot encoded: ', fc.input_layer(dict(example), [class_fc]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建输入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use entire batch since this is such a small dataset.\n",
    "NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "    # For training, cycle thru dataset as many times as need (n_epochs=None).    \n",
    "    dataset = dataset.repeat(n_epochs)  \n",
    "    # In memory training doesn't use batching.\n",
    "    dataset = dataset.batch(NUM_EXAMPLES)\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "# Training and evaluation input functions.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与评估模型\n",
    "操作步骤：\n",
    " - 初始化模型，指定特征与超参数\n",
    " - 喂给模型参数，开始训练模型\n",
    " - 评估模型性能，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估线性分类模型做为基准参考，将与提升树做对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns)\n",
    "\n",
    "# Train model.\n",
    "linear_est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "# Evaluation.\n",
    "results = linear_est.evaluate(eval_input_fn)\n",
    "print('Accuracy : ', results['accuracy'])\n",
    "print('Dummy model: ', results['accuracy_baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估提升树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since data fits into memory, use entire dataset per layer. It will be faster.\n",
    "# Above one batch is defined as the entire dataset. \n",
    "n_batches = 1\n",
    "est = tf.estimator.BoostedTreesClassifier(feature_columns,\n",
    "                                          n_batches_per_layer=n_batches)\n",
    "\n",
    "# The model will stop training once the specified number of trees is built, not \n",
    "# based on the number of steps.\n",
    "est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "# Eval.\n",
    "results = est.evaluate(eval_input_fn)\n",
    "print('Accuracy : ', results['accuracy'])\n",
    "print('Dummy model: ', results['accuracy_baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出于性能方面考虑，推荐使用`boosted_trees_classifier_train_in_memory`方法\n",
    "此方法时，不应该对输入数据进行批处理，因为该方法对整个数据集进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inmemory_train_input_fn(X, y):\n",
    "  def input_fn():\n",
    "    return dict(X), y\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "train_input_fn = make_inmemory_train_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\n",
    "est = tf.contrib.estimator.boosted_trees_classifier_train_in_memory(\n",
    "    train_input_fn,\n",
    "    feature_columns)\n",
    "print(est.evaluate(eval_input_fn)['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow模型经过优化，可以同时对一批或多个示例进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dicts = list(est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "\n",
    "probs.plot(kind='hist', bins=20, title='predicted probabilities');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您还可以查看结果的接收者操作特性(ROC)，这将使我们更好地了解真实阳性率和假阳性率之间的权衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_eval, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.xlim(0,)\n",
    "plt.ylim(0,);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
